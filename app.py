import streamlit as st
import streamlit.components.v1 as components
import requests
import json
import pandas as pd
import time
import concurrent.futures
import re
import math
import csv

#######################################
# 1) –ù–ê–°–¢–†–û–ô–ö–ò –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
#######################################

# –ë–∞–∑–æ–≤—ã–π URL API Novita
API_BASE_URL = "https://api.novita.ai/v3/openai"
LIST_MODELS_ENDPOINT = f"{API_BASE_URL}/models"
CHAT_COMPLETIONS_ENDPOINT = f"{API_BASE_URL}/chat/completions"

# –ö–ª—é—á –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–ù–ï–ë–ï–ó–û–ü–ê–°–ù–û –≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–¥–µ)
DEFAULT_API_KEY = "sk_MyidbhnT9jXzw-YDymhijjY8NF15O0Qy7C36etNTAxE"

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ 429 (Rate Limit)
MAX_RETRIES = 3

st.set_page_config(page_title="üß† Novita AI Batch Processor", layout="wide")

#######################################
# 2) –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
#######################################

def custom_postprocess_text(text: str) -> str:
    """
    –§—É–Ω–∫—Ü–∏—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞:
    1. –£–¥–∞–ª—è–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å "Note:".
    2. –£–¥–∞–ª—è–µ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ "fucking", "explicit" –∏ "intense", –µ—Å–ª–∏ –æ–Ω–∏ –ø–æ—è–≤–ª—è—é—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
    3. –ó–∞–º–µ–Ω—è–µ—Ç —Ü–µ–Ω–∑—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ "F***" –Ω–∞ "fuck".
    4. –£–¥–∞–ª—è–µ—Ç –∫–∏—Ç–∞–π—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã.
    5. –£–¥–∞–ª—è–µ—Ç —ç–º–æ–¥–∑–∏.
    6. –£–±–∏—Ä–∞–µ—Ç –≤—Å–µ –¥–≤–æ–π–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã.
    """
    text = re.sub(r'\s*Note:.*', '', text, flags=re.IGNORECASE)
    pattern_sentence = re.compile(r'(^|(?<=[.!?]\s))\s*(?:fucking|explicit|intense)[\s,:\-]+', flags=re.IGNORECASE)
    text = pattern_sentence.sub(r'\1', text)
    text = re.sub(r'\bF\*+\b', 'fuck', text, flags=re.IGNORECASE)
    text = re.sub(r'[\u4e00-\u9fff]+', '', text)
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"
                               u"\U0001F300-\U0001F5FF"
                               u"\U0001F680-\U0001F6FF"
                               u"\U0001F1E0-\U0001F1FF"
                               "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)
    text = text.replace('"', '')
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def get_model_list(api_key: str):
    """–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ API Novita AI"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    try:
        resp = requests.get(LIST_MODELS_ENDPOINT, headers=headers)
        if resp.status_code == 200:
            data = resp.json()
            models = [m["id"] for m in data.get("data", [])]
            return models
        else:
            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π. –ö–æ–¥: {resp.status_code}. –¢–µ–∫—Å—Ç: {resp.text}")
            return []
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
        return []

def chat_completion_request(
    api_key: str,
    messages: list,
    model: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    min_p: float,
    top_k: int,
    presence_penalty: float,
    frequency_penalty: float,
    repetition_penalty: float
):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π chat-–∫–æ–º–ø–ª–∏—à–Ω —Å retries –ø—Ä–∏ 429."""
    payload = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "presence_penalty": presence_penalty,
        "frequency_penalty": frequency_penalty,
        "repetition_penalty": repetition_penalty,
        "min_p": min_p
    }
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    attempts = 0
    while attempts < MAX_RETRIES:
        attempts += 1
        try:
            resp = requests.post(CHAT_COMPLETIONS_ENDPOINT, headers=headers, data=json.dumps(payload))
            if resp.status_code == 200:
                data = resp.json()
                return data["choices"][0]["message"].get("content", "")
            elif resp.status_code == 429:
                time.sleep(2)
                continue
            else:
                return f"–û—à–∏–±–∫–∞: {resp.status_code} - {resp.text}"
        except Exception as e:
            return f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}"
    return "–û—à–∏–±–∫–∞: –ü—Ä–µ–≤—ã—à–µ–Ω–æ —á–∏—Å–ª–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ 429 RATE_LIMIT."

def process_single_row(
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    row_text: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    min_p: float,
    top_k: int,
    presence_penalty: float,
    frequency_penalty: float,
    repetition_penalty: float
):
    """–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{user_prompt}\n{row_text}"}
    ]
    raw_response = chat_completion_request(
        api_key,
        messages,
        model,
        max_tokens,
        temperature,
        top_p,
        min_p,
        top_k,
        presence_penalty,
        frequency_penalty,
        repetition_penalty
    )
    final_response = custom_postprocess_text(raw_response)
    return final_response

def process_file(
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    df: pd.DataFrame,
    title_col: str,
    response_format: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    min_p: float,
    top_k: int,
    presence_penalty: float,
    frequency_penalty: float,
    repetition_penalty: float,
    chunk_size: int = 10,
    max_workers: int = 5
):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —á–∞–Ω–∫–æ–≤.
    –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –∏ –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è.
    """
    results = []
    total_rows = len(df)
    progress_bar = st.progress(0)
    time_placeholder = st.empty()  # –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏
    overall_start = time.time()
    rows_processed = 0

    for start_idx in range(0, total_rows, chunk_size):
        chunk_start = time.time()
        end_idx = min(start_idx + chunk_size, total_rows)
        chunk_indices = list(df.index[start_idx:end_idx])
        chunk_results = [None] * len(chunk_indices)

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_i = {}
            for i, row_idx in enumerate(chunk_indices):
                row_text = str(df.loc[row_idx, title_col])
                future = executor.submit(
                    process_single_row,
                    api_key,
                    model,
                    system_prompt,
                    user_prompt,
                    row_text,
                    max_tokens,
                    temperature,
                    top_p,
                    min_p,
                    top_k,
                    presence_penalty,
                    frequency_penalty,
                    repetition_penalty
                )
                future_to_i[future] = i
            for future in concurrent.futures.as_completed(future_to_i):
                i = future_to_i[future]
                chunk_results[i] = future.result()

        results.extend(chunk_results)
        rows_processed += len(chunk_indices)
        progress_bar.progress(min(rows_processed / total_rows, 1.0))

        # –†–∞—Å—á–µ—Ç –æ—Å—Ç–∞–≤—à–µ–≥–æ—Å—è –≤—Ä–µ–º–µ–Ω–∏
        chunk_time = time.time() - chunk_start
        if len(chunk_indices) > 0:
            time_per_row = chunk_time / len(chunk_indices)
            rows_left = total_rows - rows_processed
            est_time_sec = rows_left * time_per_row
            if est_time_sec < 60:
                time_text = f"~{est_time_sec:.1f} —Å–µ–∫."
            else:
                time_text = f"~{est_time_sec/60:.1f} –º–∏–Ω."
            time_placeholder.info(f"–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: {time_text}")

    overall_time = time.time() - overall_start
    time_placeholder.success(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {overall_time:.1f} —Å–µ–∫.")
    df_out = df.copy()
    df_out["rewrite"] = results
    return df_out

# --- –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞, —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º) ---

def translate_completion_request(
    api_key: str,
    messages: list,
    model: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    min_p: float,
    top_k: int,
    presence_penalty: float,
    frequency_penalty: float,
    repetition_penalty: float
):
    raw_response = chat_completion_request(
        api_key,
        messages,
        model,
        max_tokens,
        temperature,
        top_p,
        min_p,
        top_k,
        presence_penalty,
        frequency_penalty,
        repetition_penalty
    )
    final_response = custom_postprocess_text(raw_response)
    return final_response

def process_translation_single_row(
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    row_text: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    min_p: float,
    top_k: int,
    presence_penalty: float,
    frequency_penalty: float,
    repetition_penalty: float
):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{user_prompt}\n{row_text}"}
    ]
    translated_text = translate_completion_request(
        api_key=api_key,
        messages=messages,
        model=model,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        min_p=min_p,
        top_k=top_k,
        presence_penalty=presence_penalty,
        frequency_penalty=frequency_penalty,
        repetition_penalty=repetition_penalty
    )
    return translated_text

def process_translation_file(
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    df: pd.DataFrame,
    title_col: str,
    max_tokens: int,
    temperature: float,
    top_p: float,
    min_p: float,
    top_k: int,
    presence_penalty: float,
    frequency_penalty: float,
    repetition_penalty: float,
    chunk_size: int = 10,
    max_workers: int = 5
):
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —á–∞–Ω–∫–æ–≤.
    –û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –∏ –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è.
    """
    results = []
    total_rows = len(df)
    progress_bar = st.progress(0)
    time_placeholder = st.empty()
    overall_start = time.time()
    rows_processed = 0

    for start_idx in range(0, total_rows, chunk_size):
        chunk_start = time.time()
        end_idx = min(start_idx + chunk_size, total_rows)
        chunk_indices = list(df.index[start_idx:end_idx])
        chunk_results = [None] * len(chunk_indices)

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_i = {}
            for i, row_idx in enumerate(chunk_indices):
                row_text = str(df.loc[row_idx, title_col])
                future = executor.submit(
                    process_translation_single_row,
                    api_key,
                    model,
                    system_prompt,
                    user_prompt,
                    row_text,
                    max_tokens,
                    temperature,
                    top_p,
                    min_p,
                    top_k,
                    presence_penalty,
                    frequency_penalty,
                    repetition_penalty
                )
                future_to_i[future] = i
            for future in concurrent.futures.as_completed(future_to_i):
                i = future_to_i[future]
                chunk_results[i] = future.result()

        results.extend(chunk_results)
        rows_processed += len(chunk_indices)
        progress_bar.progress(min(rows_processed / total_rows, 1.0))

        chunk_time = time.time() - chunk_start
        if len(chunk_indices) > 0:
            time_per_row = chunk_time / len(chunk_indices)
            rows_left = total_rows - rows_processed
            est_time_sec = rows_left * time_per_row
            if est_time_sec < 60:
                time_text = f"~{est_time_sec:.1f} —Å–µ–∫."
            else:
                time_text = f"~{est_time_sec/60:.1f} –º–∏–Ω."
            time_placeholder.info(f"–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –æ—Å—Ç–∞–≤—à–µ–µ—Å—è –≤—Ä–µ–º—è: {time_text}")

    overall_time = time.time() - overall_start
    time_placeholder.success(f"–ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {overall_time:.1f} —Å–µ–∫.")
    df_out = df.copy()
    df_out["translated_title"] = results
    return df_out

# --- –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –≤—Ä–µ–¥–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ ---
def clean_text(text: str, harmful_patterns: list) -> str:
    """
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ —É–¥–∞–ª—è–µ—Ç –µ–≥–æ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞.
    """
    for pattern in harmful_patterns:
        text = re.sub(re.escape(pattern), "", text, flags=re.IGNORECASE)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def process_postprocessing_file(df: pd.DataFrame, text_col: str, harmful_patterns: list):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç DataFrame, –ø—Ä–∏–º–µ–Ω—è—è –æ—á–∏—Å—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º."""
    cleaned_texts = df[text_col].astype(str).apply(lambda txt: clean_text(txt, harmful_patterns))
    df_out = df.copy()
    df_out["cleaned"] = cleaned_texts
    return df_out

#######################################
# 3) –ü–†–ï–°–ï–¢–´ –ú–û–î–ï–õ–ï–ô
#######################################

PRESETS = {
    "Default": {
        "system_prompt": "Act like you are a helpful assistant.",
        "max_tokens": 512,
        "temperature": 0.70,
        "top_p": 1.0,
        "min_p": 0.0,
        "top_k": 40,
        "presence_penalty": 0.0,
        "frequency_penalty": 0.0,
        "repetition_penalty": 1.0
    },
    "NSFW": {
        "system_prompt": "You are an advanced NSFW content rewriter and evaluator. Generate one vivid and explicit title based on the input, ensuring it stays within 90 characters. The title should align with NSFW standards, SEO relevance, and native fluency.",
        "max_tokens": 32000,
        "temperature": 0.70,
        "top_p": 1.0,
        "min_p": 0.0,
        "top_k": 40,
        "presence_penalty": 0.20,
        "frequency_penalty": 0.40,
        "repetition_penalty": 1.22
    },
    "Adult_Content_Generator": {
        "system_prompt": "You are a professional content creator specializing in adult NSFW content. Generate creative and engaging content based on the input provided.",
        "max_tokens": 2500,
        "temperature": 0.85,
        "top_p": 0.95,
        "min_p": 0.0,
        "top_k": 50,
        "presence_penalty": 0.30,
        "frequency_penalty": 0.50,
        "repetition_penalty": 1.50
    },
    "Erotic_Story_Teller": {
        "system_prompt": "You are an expert in writing erotic stories. Generate a captivating and tasteful story based on the input provided.",
        "max_tokens": 5000,
        "temperature": 0.75,
        "top_p": 0.90,
        "min_p": 0.0,
        "top_k": 60,
        "presence_penalty": 0.25,
        "frequency_penalty": 0.35,
        "repetition_penalty": 1.30
    }
}

#######################################
# 4) –ò–ù–¢–ï–†–§–ï–ô–°
#######################################

st.title("üß† Novita AI Batch Processor")

# –ü–æ–ª–µ –≤–≤–æ–¥–∞ API Key
st.sidebar.header("üîë –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API")
api_key = st.sidebar.text_input("API Key", value=DEFAULT_API_KEY, type="password")

# –°–æ–∑–¥–∞–µ–º 7 –≤–∫–ª–∞–¥–æ–∫: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞, –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞, –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞, –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞,
# –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤, –£–¥–∞–ª–µ–Ω–∏–µ –∫–∞–≤—ã—á–µ–∫, –ê–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ–≥–æ–≤
tabs = st.tabs([
    "üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞",
    "üåê –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞",
    "üìÇ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞",
    "üßπ –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞",
    "üóÇÔ∏è –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤",
    "‚ùå –£–¥–∞–ª–µ–Ω–∏–µ –∫–∞–≤—ã—á–µ–∫",
    "üè∑Ô∏è –¢–µ–≥–∏"
])

########################################
# –í–∫–ª–∞–¥–∫–∞ 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
########################################
with tabs[0]:
    st.header("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞")
    # ... (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –≤–∫–ª–∞–¥–∫–∏ 0 –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

########################################
# –í–∫–ª–∞–¥–∫–∞ 2: –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞
########################################
with tabs[1]:
    st.header("üåê –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞")
    # ... (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –≤–∫–ª–∞–¥–∫–∏ 1 –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

########################################
# –í–∫–ª–∞–¥–∫–∞ 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
########################################
with tabs[2]:
    st.header("üìÇ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞")
    # ... (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –≤–∫–ª–∞–¥–∫–∏ 2 –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

########################################
# –í–∫–ª–∞–¥–∫–∞ 4: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
########################################
with tabs[3]:
    st.header("üßπ –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞")
    # ... (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –≤–∫–ª–∞–¥–∫–∏ 3 –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

########################################
# –í–∫–ª–∞–¥–∫–∞ 5: –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ CSV
########################################
with tabs[4]:
    st.header("üóÇÔ∏è –í—ã–±–æ—Ä —Å—Ç–æ–ª–±—Ü–æ–≤ –∏–∑ CSV")
    # ... (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –≤–∫–ª–∞–¥–∫–∏ 4 –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

########################################
# –í–∫–ª–∞–¥–∫–∞ 6: –£–¥–∞–ª–µ–Ω–∏–µ –∫–∞–≤—ã—á–µ–∫ –∏–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
########################################
with tabs[5]:
    st.header("‚ùå –£–¥–∞–ª–µ–Ω–∏–µ –∫–∞–≤—ã—á–µ–∫")
    # ... (–≤–∞—à —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –≤–∫–ª–∞–¥–∫–∏ 5 –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π) ...

########################################
# –í–∫–ª–∞–¥–∫–∞ 7: –ê–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤
########################################
with tabs[6]:
    st.header("üè∑Ô∏è –ê–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–µ–≥–æ–≤")
    st.write("1) –í–≤–µ–¥–∏—Ç–µ —Å–ø–∏—Å–æ–∫ —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã—Ö —Ç–µ–≥–æ–≤ (Allowed Tags).")
    tags_input = st.text_area(
        "Allowed Tags (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é –∏–ª–∏ –ø–æ —Å—Ç—Ä–æ–∫–∞–º):",
        height=150,
        placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä: wellness, health, beauty, fitness, anti-aging..."
    )
    user_tags = [t.strip() for t in re.split(r'[\n,]+', tags_input) if t.strip()]

    st.write("2) –ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å –∫–æ–ª–æ–Ω–∫–æ–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–µ–≥–æ–≤.")
    uploaded = st.file_uploader("CSV-—Ñ–∞–π–ª", type="csv", key="tags_csv")
    if uploaded and user_tags:
        df_tags = pd.read_csv(uploaded)
        cols = df_tags.columns.tolist()
        tag_col = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ç–µ–≥–∞–º–∏", cols, key="tag_col")
        max_workers_tags = st.slider("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤", 1, 10, 5, key="max_workers_tags")

        if st.button("‚ñ∂Ô∏è –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–≥–∏", key="process_tags"):
            def process_tags_row(row):
                existing = [t.strip() for t in re.split(r'[;,]', str(row[tag_col])) if t.strip()]
                context = row[tag_col]  # –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –∫–æ–ª–æ–Ω–∫—É –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
                allowed = ", ".join(user_tags)

                system_msg = "You are an expert tag selector."
                user_msg = f"""
Allowed tags: {allowed}.
Existing tags: {', '.join(existing)}.
Context: {context}

Task: From the Allowed tags list, pick exactly 5 tags that best describe the Context.
- Always include any Existing tags that are relevant.
- If there are fewer than 5 relevant Existing tags, add other Allowed tags that fit the Context to reach exactly 5.
- Do NOT choose tags outside the Allowed list.
Return only a comma-separated list of the 5 tags.
"""
                resp = chat_completion_request(
                    api_key,
                    [
                        {"role": "system", "content": system_msg},
                        {"role": "user",   "content": user_msg}
                    ],
                    selected_model_text,
                    max_tokens_text,
                    temperature_text,
                    top_p_text,
                    min_p_text,
                    top_k_text,
                    presence_penalty_text,
                    frequency_penalty_text,
                    repetition_penalty_text
                )
                tags_out = custom_postprocess_text(resp)
                return [t.strip() for t in tags_out.split(",")][:5]

            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_tags) as executor:
                augmented = list(executor.map(process_tags_row, [row for _, row in df_tags.iterrows()]))

            df_tags["final_5_tags"] = [", ".join(tlist) for tlist in augmented]
            st.success("‚úÖ –¢–µ–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!")
            st.dataframe(df_tags.head(10))

            csv_out = df_tags.to_csv(index=False).encode("utf-8")
            st.download_button(
                "üì• –°–∫–∞—á–∞—Ç—å CSV —Å 5 —Ç–µ–≥–∞–º–∏",
                data=csv_out,
                file_name="tags_with_5.csv",
                mime="text/csv"
            )
    elif uploaded and not user_tags:
        st.warning("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —Ä–∞–∑—Ä–µ—à—ë–Ω–Ω—ã–π —Ç–µ–≥ –≤—ã—à–µ.")


